{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b67a6652-66ab-4899-8701-310a86935fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import pipeline\n",
    "from unsloth import FastLanguageModel\n",
    "from typing import Dict\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436b7fe3-99cb-4e71-869c-830b87d988b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f6bb6e4-cd73-4962-90e3-d92be93690f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is a high school diploma required for an F-1 v...</td>\n",
       "      <td>A high school diploma or its equivalent is gen...</td>\n",
       "      <td>Question Understanding\\nThe question asks whet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is it important to memorize my SEVIS ID?</td>\n",
       "      <td>It's crucial to know your SEVIS ID, as it's yo...</td>\n",
       "      <td>Question Understanding\\nThe question asks abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is proof of housing required at the port of en...</td>\n",
       "      <td>While proof of housing is not always required ...</td>\n",
       "      <td>Question Understanding\\nThe question asks whet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What document does a school provide for an F-1...</td>\n",
       "      <td>A school provides Form I-20, a Certificate of ...</td>\n",
       "      <td>Question Understanding\\nThe question asks abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What if I plan to do research collaboration wi...</td>\n",
       "      <td>If asked about potential research collaboratio...</td>\n",
       "      <td>Question Understanding\\nThe question concerns ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  Is a high school diploma required for an F-1 v...   \n",
       "1           Is it important to memorize my SEVIS ID?   \n",
       "2  Is proof of housing required at the port of en...   \n",
       "3  What document does a school provide for an F-1...   \n",
       "4  What if I plan to do research collaboration wi...   \n",
       "\n",
       "                                              Answer  \\\n",
       "0  A high school diploma or its equivalent is gen...   \n",
       "1  It's crucial to know your SEVIS ID, as it's yo...   \n",
       "2  While proof of housing is not always required ...   \n",
       "3  A school provides Form I-20, a Certificate of ...   \n",
       "4  If asked about potential research collaboratio...   \n",
       "\n",
       "                                           reasoning  \n",
       "0  Question Understanding\\nThe question asks whet...  \n",
       "1  Question Understanding\\nThe question asks abou...  \n",
       "2  Question Understanding\\nThe question asks whet...  \n",
       "3  Question Understanding\\nThe question asks abou...  \n",
       "4  Question Understanding\\nThe question concerns ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../dataset/cleaned_dataset_answer_improved_reasoned.csv\", encoding=\"utf-8\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b32deb63-60e6-461d-aa50-368741d5c02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c17816bf-0821-41fd-8b62-9ca06adaa072",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Jenitza182/Meta-Llama-3.1-8B-Instruct-law-lora_model'  # Jenitza182/Meta-Llama-3.1-8B-Instruct-law-lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2d9e0cb-48b1-4ba3-bb3e-1d0666f61ddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.381 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "          (1): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "          (2-31): 30 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_name,\n",
    "        max_seq_length = 8192,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "        )\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ada08b2-45de-42d6-9839-37576a9f97a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTER_TO_INDEX = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "\n",
    "class Example:\n",
    "    def __init__(self, question: str, choice1: str, choice2: str, choice3: str, choice4: str):\n",
    "        self.question = question\n",
    "        self.choice1 = choice1\n",
    "        self.choice2 = choice2\n",
    "        self.choice3 = choice3\n",
    "        self.choice4 = choice4\n",
    "\n",
    "    @property\n",
    "    def choices(self):\n",
    "        return [self.choice1, self.choice2, self.choice3, self.choice4]\n",
    "\n",
    "def _base_prompt() -> str:\n",
    "    return \"\"\"Act as an expert legal assistant with comprehensive knowledge of statutory law and case precedent. Analyze the following legal question carefully, then select the correct answer from the given options through rigorous legal reasoning.\"\"\"\n",
    "\n",
    "def _format_choices(options: list[str]) -> str:\n",
    "    return \"\\n\".join(f\"({chr(65 + i)}) {choice}\" for i, choice in enumerate(options))\n",
    "\n",
    "def _build_question_section(example: Example) -> str:\n",
    "    return f\"\\n\\nQuestion: {example.question}\\nChoices:\\n{_format_choices(example.choices)}\"\n",
    "\n",
    "def _build_instructions() -> str:\n",
    "    return \"\"\"\\n\\n\n",
    "        Instructions:\n",
    "        1. Conduct thorough legal analysis of all options\n",
    "        2. Consider relevant statutes, regulations, and judicial interpretations\n",
    "        3. Identify potential ambiguities or counterarguments\n",
    "        4. Select only the BEST supported answer\n",
    "        5. Respond SOLELY with the correct letter (A-D)\n",
    "\n",
    "        Answer using this format:\n",
    "        [X]\"\"\"\n",
    "\n",
    "def _build_final_instruction() -> str:\n",
    "    return \"\\n\\nPlease reply only with the correct option, do not say anything else.\"\n",
    "\n",
    "def _prepare_examples(example: Example, no: int = 5) -> str:\n",
    "    filtered_df = df[df['Question'] != example.question].sample(frac=1)\n",
    "    examples = []\n",
    "    \n",
    "    for _, row in filtered_df.head(no).iterrows():\n",
    "        right_answer = str(row['Answer'])\n",
    "        option = [right_answer]\n",
    "        \n",
    "        distractors = df[df['Answer'] != right_answer]['Answer'].astype(str).unique()\n",
    "        if len(distractors) < 3:\n",
    "            raise ValueError(\"Not enough unique distractors in the DataFrame.\")\n",
    "        option += random.sample(list(distractors), 3)\n",
    "        \n",
    "        random.shuffle(option)\n",
    "        correct_letter = chr(option.index(right_answer) + 65)\n",
    "        \n",
    "        example_str = (\n",
    "            f\"\\n\\nQuestion: {row['Question']}\"\n",
    "            f\"\\nChoices:\\n{_format_choices(option)}\"\n",
    "            f\"\\nThe correct answer is ({correct_letter})\"\n",
    "        )\n",
    "        examples.append(example_str)\n",
    "    \n",
    "    return \"--- START OF EXAMPLES ---\\n\" + \"\".join(examples) + \"\\n\\n--- END OF EXAMPLES ---\\n\"\n",
    "\n",
    "def chain_of_thought_prompt(example: Example, max_new_tokens: int = 256) -> str:\n",
    "    prompt = _base_prompt() + _build_question_section(example)\n",
    "    prompt += f\"\\n\\nLet's analyze this step by step. First, understand the question. Next, evaluate each option in short (2-5) lines each. Also, you can generate up to {max_new_tokens} tokens to reason.\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt},]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,  # Must add for generation\n",
    "                return_tensors=\"pt\",).to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "                input_ids=inputs, \n",
    "                max_new_tokens=max_new_tokens, \n",
    "                use_cache=True, \n",
    "                temperature=1.5, \n",
    "                min_p=0.1)\n",
    "\n",
    "\n",
    "    # Extract generated text\n",
    "    cot_reasoning = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    prompt = f\"{cot_reasoning}\\n\\nBased on the above, reasoning what is the single, most likely answer choice? \\nAnswer using this format:\\n[X]\"\n",
    "    prompt += _build_final_instruction()\n",
    "    return prompt\n",
    "\n",
    "def five_shot_prompt(example: Example, no: int = 5) -> str:\n",
    "    prompt = _base_prompt()\n",
    "    prompt += \"\\nHere are some example questions from experts. Answer the final question yourself, following the format of the previous questions exactly.\\n\"\n",
    "    prompt += _prepare_examples(example=example, no=no)\n",
    "    prompt += \"\\n\\nNow your turn. Choose the correct option that answers the below question.\\n\"\n",
    "    prompt += _build_question_section(example)\n",
    "    prompt += _build_instructions()\n",
    "    prompt += _build_final_instruction()\n",
    "    return prompt\n",
    "\n",
    "def zero_shot_prompt(example: Example) -> str:\n",
    "    prompt = _base_prompt() + _build_question_section(example)\n",
    "    prompt += _build_instructions()\n",
    "    prompt += _build_final_instruction()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac83829c-ce8e-4c95-ad16-d2cdded0b954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed9272b0-b2f2-4c52-b547-e938920c44f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(n=1, prompt_type='zero_shot_prompt'):\n",
    "    for i in tqdm(range(n), desc=\"Iterations\"):\n",
    "        results = []\n",
    "        \n",
    "        # Precompute paths and create directory once per iteration\n",
    "        model_folder = model_name.split('/')[1]\n",
    "        output_dir = f\"../results/finetune_model_eval/{prompt_type}/{model_folder}/\"\n",
    "        output_file = os.path.join(output_dir, f\"output_{i}.json\")\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Create directory if needed\n",
    "        \n",
    "        for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
    "            try:\n",
    "                question = row['Question']\n",
    "                right_answer = row['Answer']\n",
    "                # selecting distractions.\n",
    "                option = []\n",
    "                option.append(right_answer)\n",
    "                while len(option) < 4:\n",
    "                    distractor = df.sample(1)['Answer'].values[0]\n",
    "                    if distractor not in option and distractor != right_answer:\n",
    "                        option.append(distractor)\n",
    "\n",
    "                # Create an example\n",
    "                random.shuffle(option)\n",
    "                right_option_index = option.index(right_answer)\n",
    "                right_option_letter = chr(ord('A') + right_option_index)\n",
    "                \n",
    "                example = Example(question, \n",
    "                                  option[0], \n",
    "                                  option[1], \n",
    "                                  option[2], \n",
    "                                  option[3]\n",
    "                                )\n",
    "                \n",
    "                # Depending on prompt_type, generate the prompt using the integrated functions\n",
    "                if prompt_type == 'zero_shot_prompt':\n",
    "                    prompt = zero_shot_prompt(example)\n",
    "                elif prompt_type == 'five_shot_prompt':\n",
    "                    n_examples = 5\n",
    "                    prompt =  five_shot_prompt(example, n_examples)\n",
    "                elif prompt_type == 'chain_of_thought_prompt':\n",
    "                    thinking_tokens = 512\n",
    "                    prompt = chain_of_thought_prompt(example, thinking_tokens)\n",
    "                else:\n",
    "                    # Default behavior (original prompt)\n",
    "                    print(\"SELECTING DEFAULT PROMPTING TECHNIQUE\")\n",
    "                    prompt = zero_shot_prompt(example)\n",
    "\n",
    "                # Prepare Inputs\n",
    "                messages = [{\"role\": \"user\", \"content\": prompt},]\n",
    "                inputs = tokenizer.apply_chat_template(\n",
    "                            messages,\n",
    "                            tokenize=True,\n",
    "                            add_generation_prompt=True,  # Must add for generation\n",
    "                            return_tensors=\"pt\",).to(\"cuda\")\n",
    "                \n",
    "                # Generate outputs\n",
    "                outputs = model.generate(\n",
    "                            input_ids=inputs, \n",
    "                            max_new_tokens=15, # Restrict to 15 tokens so that we dont get garbage\n",
    "                            use_cache=True, \n",
    "                            temperature=1.5, \n",
    "                            min_p=0.1)\n",
    "\n",
    "\n",
    "                # Decode output\n",
    "                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                answer_portion = generated_text.split(\"do not say anything else.\")[1] # do not say anything else. : is the ending of our prompt\n",
    "                \n",
    "                match = re.search(r'[A-D]', answer_portion)\n",
    "                if match:\n",
    "                    model_answer_letter = match.group()\n",
    "                else:\n",
    "                    model_answer_letter = 'NA'\n",
    "                \n",
    "                if index == 0 and i==0:\n",
    "                    print(generated_text)\n",
    "                    print(\"Correct Ans: \", right_answer)\n",
    "                    print(\"Model Replied with: \", model_answer_letter)\n",
    "                    \n",
    "                is_correct = (right_option_letter and model_answer_letter) and right_option_letter.lower() == model_answer_letter.lower()\n",
    "                result = {\n",
    "                    \"iteration\": i,\n",
    "                    \"id\": index,\n",
    "                    \"model\": model_name,\n",
    "                    \"prompt_type\": prompt_type,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"question\": question,\n",
    "                    \"right_answer\": right_answer,\n",
    "                    \"right_answer_option\": right_option_letter,\n",
    "                    \"model_answer_letter\": model_answer_letter,\n",
    "                    \"generated_text\": generated_text,\n",
    "                    \"answer_portion\": answer_portion,\n",
    "                    \"is_correct\": str(is_correct)\n",
    "                }\n",
    "                results.append(result)\n",
    "\n",
    "                # Save progress every 50 successful completions\n",
    "                if len(results) % 50 == 0:\n",
    "                    #print('saving')\n",
    "                    with open(output_file, 'w') as f:\n",
    "                        json.dump(results, f, indent=4)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {index}: {e}\")\n",
    "                continue \n",
    "\n",
    "        # Final save for remaining items after processing all rows\n",
    "        if len(results) > 0:\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "                \n",
    "        print(f\"Iteration {i} complete. Results saved to {output_file}\")\n",
    "        print(\"Evaluation complete. Results saved to output.json.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cd62c74-9d2c-470c-999c-c0c18a9ce9e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303faaa273494d6598bc385c349ae2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3714e686f6343f2997916064f396df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing rows:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "user\n",
      "\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "user\n",
      "\n",
      "Act as an expert legal assistant with comprehensive knowledge of statutory law and case precedent. Analyze the following legal question carefully, then select the correct answer from the given options through rigorous legal reasoning.\n",
      "\n",
      "Question: Is a high school diploma required for an F-1 visa?\n",
      "Choices:\n",
      "(A) Upon changing employers during your STEM OPT, you must report the change within 10 days to your Designated School Official (DSO). This report should include a new I-983 training plan completed with your new employer, who must be enrolled in E-Verify. The new position must also be a bonafide job offer and directly related to your STEM degree.\n",
      "(B) Yes, you can hold multiple CPT positions, but each job must directly relate to your field of study and be authorized separately by your university's Designated School Official (DSO). You'll need to obtain CPT authorization for each employer, ensuring that each work experience meets the academic requirements and is properly documented in SEVIS. Failure to secure individual authorizations for each job could jeopardize your F-1 student status.\n",
      "(C) Reapplying for OPT after a denial doesn't automatically jeopardize your F-1 status, as long as you maintain compliance with all other F-1 requirements. However, it's crucial to analyze the reason for the initial denial and address any underlying issues before resubmitting your application. Repeated denials could raise concerns about your eligibility and potentially affect your status, so seeking guidance from a Designated School Official (DSO) is highly recommended.\n",
      "(D) A high school diploma or its equivalent is generally required for an F-1 visa if you intend to pursue academic studies at a college, university, or other post-secondary institution. However, if you plan to enroll in a vocational or non-academic program, a high school diploma might not be mandatory, but meeting the educational requirements of that specific program is still essential. Always verify the specific admission requirements of the educational institution you plan to attend to ensure compliance with F-1 visa eligibility criteria.\n",
      "\n",
      "Let's analyze this step by step. First, understand the question. Next, evaluate each option in short (2-5) lines each. Also, you can generate up to 512 tokens to reason.assistant\n",
      "\n",
      "To determine if a high school diploma is required for an F-1 visa, consider the type of program intended. For academic studies at colleges, universities, or post-secondary institutions, a high school diploma or equivalent is generally mandated. However, vocational or non-academic programs may have different requirements, so verifying the specific admission criteria of the educational institution is essential to ensure compliance with F-1 visa eligibility. Therefore, option (D) provides the most accurate and complete explanation of the high school diploma requirement for an F-1 visa.\n",
      "\n",
      "Based on the above, reasoning what is the single, most likely answer choice? \n",
      "Answer using this format:\n",
      "[X]\n",
      "\n",
      "Please reply only with the correct option, do not say anything else.assistant\n",
      "\n",
      "[D]\n",
      "Correct Ans:  A high school diploma or its equivalent is generally required for an F-1 visa if you intend to pursue academic studies at a college, university, or other post-secondary institution. However, if you plan to enroll in a vocational or non-academic program, a high school diploma might not be mandatory, but meeting the educational requirements of that specific program is still essential. Always verify the specific admission requirements of the educational institution you plan to attend to ensure compliance with F-1 visa eligibility criteria.\n",
      "Model Replied with:  D\n",
      "Iteration 0 complete. Results saved to ../results/finetune_model_eval/chain_of_thought_prompt/Meta-Llama-3.1-8B-Instruct-law-lora_model/output_0.json\n",
      "Evaluation complete. Results saved to output.json.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458883be5ec2423eb7bbfaf2e5f3c90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing rows:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 complete. Results saved to ../results/finetune_model_eval/chain_of_thought_prompt/Meta-Llama-3.1-8B-Instruct-law-lora_model/output_1.json\n",
      "Evaluation complete. Results saved to output.json.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc0e6fc5f764a16b1ba798d63c910c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing rows:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2 complete. Results saved to ../results/finetune_model_eval/chain_of_thought_prompt/Meta-Llama-3.1-8B-Instruct-law-lora_model/output_2.json\n",
      "Evaluation complete. Results saved to output.json.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44be27a4018d49129e6213a3e89c77ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing rows:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3 complete. Results saved to ../results/finetune_model_eval/chain_of_thought_prompt/Meta-Llama-3.1-8B-Instruct-law-lora_model/output_3.json\n",
      "Evaluation complete. Results saved to output.json.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8043d2b6a9b4a359608730c90cb534a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing rows:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4 complete. Results saved to ../results/finetune_model_eval/chain_of_thought_prompt/Meta-Llama-3.1-8B-Instruct-law-lora_model/output_4.json\n",
      "Evaluation complete. Results saved to output.json.\n"
     ]
    }
   ],
   "source": [
    "runner(5, 'chain_of_thought_prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d668fe35-f5e0-412b-928f-3da0a7947aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../results/finetune_model_eval/chain_of_thought_prompt/Meta-Llama-3.1-8B-Instruct-law-lora_model/output_3.json\n",
      "Total is_correct = True: 934\n",
      "Total is_correct = False: 66\n",
      "Total is_correct = NA: 0\n",
      "----------------------------------------\n",
      "File: ../results/finetune_model_eval/chain_of_thought_prompt/Meta-Llama-3.1-8B-Instruct-law-lora_model/output_1.json\n",
      "Total is_correct = True: 953\n",
      "Total is_correct = False: 47\n",
      "Total is_correct = NA: 0\n",
      "----------------------------------------\n",
      "File: ../results/finetune_model_eval/chain_of_thought_prompt/Meta-Llama-3.1-8B-Instruct-law-lora_model/output_2.json\n",
      "Total is_correct = True: 945\n",
      "Total is_correct = False: 55\n",
      "Total is_correct = NA: 0\n",
      "----------------------------------------\n",
      "File: ../results/finetune_model_eval/chain_of_thought_prompt/Meta-Llama-3.1-8B-Instruct-law-lora_model/output_4.json\n",
      "Total is_correct = True: 941\n",
      "Total is_correct = False: 59\n",
      "Total is_correct = NA: 0\n",
      "----------------------------------------\n",
      "File: ../results/finetune_model_eval/chain_of_thought_prompt/Meta-Llama-3.1-8B-Instruct-law-lora_model/output_0.json\n",
      "Total is_correct = True: 952\n",
      "Total is_correct = False: 48\n",
      "Total is_correct = NA: 0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "prompt_type = 'chain_of_thought_prompt' #\"chain_of_thought_prompt\"  # five_shot_prompt # zero_shot_prompt\n",
    "model_name =  \"Jenitza182/Meta-Llama-3.1-8B-Instruct-law-lora_model\"\n",
    "\n",
    "# Construct the directory path\n",
    "directory = f\"../results/finetune_model_eval/{prompt_type}/{model_name.split('/')[1]}/\" # When running on Jetstream\n",
    "#directory = f\"../results/finetune_model_eval/{prompt_type}/{model_name.split('/')[1]}/\" # When running in VS CODE\n",
    "\n",
    "# Find all JSON files matching output_*.json pattern\n",
    "json_files = glob.glob(os.path.join(directory, \"output_*.json\"))\n",
    "\n",
    "# Process each JSON file\n",
    "for file_path in json_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Count occurrences of is_correct = True and is_correct = False\n",
    "    true_count = sum(1 for item in data if item.get(\"is_correct\") == 'True') # model answer == actual answer\n",
    "    false_count = sum(1 for item in data if item.get(\"is_correct\") == 'False') # model answer != actual answer\n",
    "    na_count = sum(1 for item in data if item.get(\"is_correct\") == 'NA') # Model was not able to answer/ Answer not found in response\n",
    "    \n",
    "    print(f\"File: {file_path}\")\n",
    "    print(f\"Total is_correct = True: {true_count}\")\n",
    "    print(f\"Total is_correct = False: {false_count}\")\n",
    "    print(f\"Total is_correct = NA: {na_count}\")\n",
    "    print(\"-\" * 40) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67229fc7-7192-48a2-9054-1b743aa0628b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
