{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/exouser/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts proton and proton-viewer are installed in '/home/exouser/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script isympy is installed in '/home/exouser/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts torchfrtrace and torchrun are installed in '/home/exouser/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script transformers-cli is installed in '/home/exouser/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -U \"huggingface_hub[cli]\" -q\n",
    "! pip install torch -q\n",
    "! pip install transformers_stream_generator einops tiktoken pinecone -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script dotenv is installed in '/home/exouser/.local/bin' which is not on PATH.\r\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sentence_transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def get_api_key(api_name):\n",
    "    # Specify the path to your dummy.env file\n",
    "    env_path = \"../.dummy_env\"  # Adjust the path as needed\n",
    "    # Load the environment variables\n",
    "    load_dotenv(env_path)\n",
    "    return os.getenv(api_name)\n",
    "\n",
    "\n",
    "# Access the Pinecone API key\n",
    "pinecone_api_key = get_api_key(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: huggingface-cli: command not found\r\n"
     ]
    }
   ],
   "source": [
    "huggingface_token = get_api_key(\"HUGGINGFACE_API_KEY\")\n",
    "!huggingface-cli login --token huggingface_token #HUGGINGFACE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2863dbf24d82407e989f7b806cb4c8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43951d582b64406394902a874364e808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8567f25974154beaa985d87e1065c71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d46d45e4684a428fde7eef17fe4e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4cbe49994904891a2a92dbde083ac6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832a959d4b52476980334a9bfc213c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d118bf0d90104b27a484a022ec589bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca862b1871eb4bd8b051e060d0fe459a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3461ecfe6577459a928176c3b5bf7a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9e529b993c47b6891af45985adcc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5225337e7ad4459899c14095a0ea7a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "\n",
    "# Initialize Pinecone and embedding model\n",
    "def initialize_pinecone():\n",
    "    # Access the Pinecone API key\n",
    "    pinecone_api_key = get_api_key(\"PINECONE_API_KEY\")\n",
    "    pinecone = Pinecone(api_key=pinecone_api_key)\n",
    "    index_name = \"recursive-text-chunks\"\n",
    "    index = pinecone.Index(index_name)\n",
    "    return index\n",
    "\n",
    "index = initialize_pinecone()\n",
    "\n",
    "def initialize_embedding_model():\n",
    "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "embedding_model = initialize_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for query pinecone and answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self):\n",
    "        # Initialize the device and model pipeline\n",
    "        self.device = self.get_device()\n",
    "        self.pipe = self.initialize_model()\n",
    "        self.index = self.initialize_pinecone()\n",
    "        self.embedding_model = self.initialize_embedding_model()\n",
    "\n",
    "    # Function to get the best available device\n",
    "    def get_device(self):\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                torch.cuda.empty_cache()  # Clear unused memory\n",
    "                return torch.device(\"cuda\")\n",
    "            except RuntimeError:\n",
    "                print(\"GPU out of memory. Falling back to CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "    # Try initializing the model on GPU first, fallback to CPU if OOM error occurs\n",
    "    def initialize_model(self):\n",
    "        try:\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=\"Qwen/Qwen2-7B-Instruct\",\n",
    "                device=0 if self.device.type == \"cuda\" else -1\n",
    "            )\n",
    "            print(f\"Using device: {self.device}\")\n",
    "            return pipe\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                print(\"GPU ran out of memory. Clearing memory and retrying...\")\n",
    "                torch.cuda.empty_cache()\n",
    "                import gc\n",
    "                gc.collect()\n",
    "\n",
    "                # Try again on GPU\n",
    "                try:\n",
    "                    pipe = pipeline(\n",
    "                        \"text-generation\",\n",
    "                        model=\"Qwen/Qwen2-7B-Instruct\",\n",
    "                        device=0\n",
    "                    )\n",
    "                    print(\"Retrying on GPU succeeded.\")\n",
    "                    return pipe\n",
    "                except RuntimeError as e:\n",
    "                    print(\"GPU still has OOM error. Switching to CPU.\")\n",
    "                    self.device = torch.device(\"cpu\")\n",
    "                    return pipeline(\"text-generation\", model=\"Qwen/Qwen2-7B-Instruct\", device=-1)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "\n",
    "    # Method to get the API key from the .env file\n",
    "    def get_api_key(self, api_name):\n",
    "        env_path = \"../.dummy_env\"  # Adjust the path as needed\n",
    "        load_dotenv(env_path)  # Load the environment variables\n",
    "        return os.getenv(api_name)\n",
    "\n",
    "    # Initialize Pinecone and return the index\n",
    "    def initialize_pinecone(self):\n",
    "        # Access the Pinecone API key\n",
    "        pinecone_api_key = self.get_api_key(\"PINECONE_API_KEY\")\n",
    "        pinecone = Pinecone(api_key=pinecone_api_key)\n",
    "        index_name = \"recursive-text-chunks\"\n",
    "        index = pinecone.Index(index_name)\n",
    "        return index\n",
    "\n",
    "    # Initialize the embedding model\n",
    "    def initialize_embedding_model(self):\n",
    "        return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Query Pinecone\n",
    "    def query_pinecone(self, query_text):\n",
    "        # Check if the embedding model and index are initialized\n",
    "        if not hasattr(self, 'embedding_model') or not hasattr(self, 'index'):\n",
    "            raise ValueError(\"Embedding model and index must be defined before running the function.\")\n",
    "\n",
    "        # Query Pinecone to get relevant chunks\n",
    "        query_embedding = self.embedding_model.encode(query_text).tolist()\n",
    "        return self.index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "\n",
    "    # Generate answer\n",
    "    def generate_answer(self, query_text):\n",
    "        # Query Pinecone for relevant context\n",
    "        results = self.query_pinecone(query_text)\n",
    "\n",
    "        # Concatenate the top results into a context for the Llama model\n",
    "        context = \"\\n\".join([match['metadata']['text'] for match in results.get(\"matches\", [])])\n",
    "\n",
    "        # Read the reasoning prompt from file\n",
    "        with open(\"prompt.txt\", \"r\") as f:\n",
    "            reasoning_prompt = f.read().strip()\n",
    "\n",
    "        # Construct the prompt\n",
    "        prompt = (\n",
    "            f\"{reasoning_prompt}\\n\\n\"\n",
    "            \"Context:\\n\"\n",
    "            f\"{context}\\n\\n\"\n",
    "            f\"Question: {query_text}\\n\"\n",
    "        )\n",
    "\n",
    "        # Use the pipeline to generate the answer\n",
    "        generated = self.pipe(\n",
    "            prompt,\n",
    "            max_new_tokens=50,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "        # Extract the generated answer\n",
    "        full_generated_text = generated[0]['generated_text']\n",
    "        final_answer = full_generated_text[len(prompt):].strip()\n",
    "        \n",
    "        return final_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d064a8bb52499baf6f6629f1234198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU ran out of memory. Clearing memory and retrying...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103fc5ba44be4ca7b37b9ea9527ab726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU still has OOM error. Switching to CPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccf7096f8fa4772b89eb82888e74454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "rag_obj = RAG()\n",
    "\n",
    "# Example query related to F-1 OPT and CPT\n",
    "query_text = \"What is the eligibility criteria for F-1 OPT?\"\n",
    "answer = rag_obj.generate_answer(query_text)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd313ff1b2149c9ab483fd20e90c619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac67cff40b948518c9956cd55fe9b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b820f3637364c1fa76947b1006682e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2436f95dd245a18587d11ecfa1ddb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3dd9779fb04b32b71517f9ae61c17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04180d17bab54682b34afb457756771b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51be85d9f317428b8b3242c133c9b733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bf312f31b046858b324654b5cb0163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92185583b6c5490082fb1af03551e8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c88eb8a877d4a2e9a4a09dc6191bf5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019fa8fcb1334bba8c30b3947fa7d313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2995ce0eb1e74701bac917fa0abb0259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3381b691567481195895563eb42480a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU ran out of memory. Switching to CPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3614eca0a2a40d1acd3838f9f443007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Function to get the best available device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            torch.cuda.empty_cache()  # Clear unused memory\n",
    "            return torch.device(\"cuda\")\n",
    "        except RuntimeError:\n",
    "            print(\"GPU out of memory. Falling back to CPU.\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# Try initializing the model on GPU first, fallback to CPU if OOM error occurs\n",
    "try:\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"Qwen/Qwen2-7B-Instruct\",\n",
    "        device=0 if device.type == \"cuda\" else -1  # 0 for GPU, -1 for CPU\n",
    "    )\n",
    "    print(f\"Using device: {device}\")\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"GPU ran out of memory. Switching to CPU.\")\n",
    "        pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen2-7B-Instruct\", device=-1)\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG:\n",
    "    #query pinecone\n",
    "    def query_pinecone(self, query_text,top_k=5):\n",
    "        global embedding_model, index\n",
    "        if 'embedding_model' not in globals() or 'index' not in globals():\n",
    "            raise ValueError(\"Embedding model and index must be defined before running the function.\")\n",
    "\n",
    "        # Query Pinecone to get relevant chunks\n",
    "        query_embedding = embedding_model.encode(query_text).tolist()\n",
    "        return index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "\n",
    "    #Generate answer\n",
    "    def generate_answer(self, query_text):\n",
    "        results = self.query_pinecone(query_text)\n",
    "\n",
    "        # Concatenate the top results into a context for the Llama model\n",
    "        context = \"\\n\".join([match['metadata']['text'] for match in results.get(\"matches\", [])])\n",
    "\n",
    "        # Read the reasoning prompt from file (make sure prompt.txt does not include extra Q&A instructions)\n",
    "        with open(\"prompt.txt\", \"r\") as f:\n",
    "            reasoning_prompt = f.read().strip()\n",
    "        \n",
    "        # Construct the prompt with a clear instruction to output only one answer\n",
    "        prompt = (\n",
    "            f\"{reasoning_prompt}\\n\\n\"\n",
    "            \"Context:\\n\"\n",
    "            f\"{context}\\n\\n\"\n",
    "            f\"Question: {query_text}\\n\"\n",
    "            #\"Answer (only provide the answer, do not repeat the context):\"\n",
    "        )\n",
    "\n",
    "        #print(\"\\n\\nPrompt:\\n\",prompt)\n",
    "        # Use the pipeline to generate the answer\n",
    "        generated = pipe(\n",
    "            prompt,\n",
    "            max_new_tokens=100,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "        # Remove the prompt from the generated text to isolate the answer\n",
    "        full_generated_text = generated[0]['generated_text']\n",
    "        final_answer = full_generated_text[len(prompt):].strip()\n",
    "        # print(\"Generated Answer:\", final_answer)\n",
    "        return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "rag_obj = RAG()\n",
    "\n",
    "# Example query related to F-1 OPT and CPT\n",
    "query_text = \"What is the eligibility criteria for F-1 OPT?\"\n",
    "answer = rag_obj.generate_answer(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: An F-1 student who has attended an SEVP-certified college, university, conservatory, or seminary on a full-time basis for at least one academic year may be authorized for up to 12 months of OPT per education level. However, F-1 students who have one year or more of full-time curricular practical training are not eligible for OPT for that degree.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Function to get the best available device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            torch.cuda.empty_cache()  # Clear unused memory\n",
    "            return torch.device(\"cuda\")\n",
    "        except RuntimeError:\n",
    "            print(\"GPU out of memory. Falling back to CPU.\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# Try initializing the model on GPU first, fallback to CPU if OOM error occurs\n",
    "try:\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"Qwen/Qwen2-7B-Instruct\",\n",
    "        device=0 if device.type == \"cuda\" else -1  # 0 for GPU, -1 for CPU\n",
    "    )\n",
    "    print(f\"Using device: {device}\")\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"GPU ran out of memory. Switching to CPU.\")\n",
    "        pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen2-7B-Instruct\", device=-1)\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "#query pinecone\n",
    "def query_pinecone():\n",
    "    global embedding_model, index\n",
    "    if 'embedding_model' not in globals() or 'index' not in globals():\n",
    "        raise ValueError(\"Embedding model and index must be defined before running the function.\")\n",
    "\n",
    "    # Query Pinecone to get relevant chunks\n",
    "    query_embedding = embedding_model.encode(query_text).tolist()\n",
    "    return index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "\n",
    "def generate_answer():\n",
    "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "\n",
    "    # Concatenate the top results into a context for the Llama model\n",
    "    context = \"\\n\".join([match['metadata']['text'] for match in results.get(\"matches\", [])])\n",
    "\n",
    "    # Read the reasoning prompt from file (make sure prompt.txt does not include extra Q&A instructions)\n",
    "    with open(\"prompt.txt\", \"r\") as f:\n",
    "        reasoning_prompt = f.read().strip()\n",
    "    \n",
    "    # Construct the prompt with a clear instruction to output only one answer\n",
    "    prompt = (\n",
    "        f\"{reasoning_prompt}\\n\\n\"\n",
    "        \"Context:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        f\"Question: {query_text}\\n\"\n",
    "        #\"Answer (only provide the answer, do not repeat the context):\"\n",
    "    )\n",
    "\n",
    "    #print(\"\\n\\nPrompt:\\n\",prompt)\n",
    "    # Use the pipeline to generate the answer\n",
    "    generated = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=100,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    # Remove the prompt from the generated text to isolate the answer\n",
    "    full_generated_text = generated[0]['generated_text']\n",
    "    final_answer = full_generated_text[len(prompt):].strip()\n",
    "    print(\"Generated Answer:\", final_answer)\n",
    "\n",
    "# Function to query Pinecone and generate an answer with the Llama model\n",
    "def query_pinecone_and_generate_answer(query_text, top_k=5):\n",
    "    global embedding_model, index\n",
    "    if 'embedding_model' not in globals() or 'index' not in globals():\n",
    "        raise ValueError(\"Embedding model and index must be defined before running the function.\")\n",
    "\n",
    "    # Query Pinecone to get relevant chunks\n",
    "    query_embedding = embedding_model.encode(query_text).tolist()\n",
    "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "\n",
    "    # Concatenate the top results into a context for the Llama model\n",
    "    context = \"\\n\".join([match['metadata']['text'] for match in results.get(\"matches\", [])])\n",
    "\n",
    "    # Read the reasoning prompt from file (make sure prompt.txt does not include extra Q&A instructions)\n",
    "    with open(\"prompt.txt\", \"r\") as f:\n",
    "        reasoning_prompt = f.read().strip()\n",
    "    \n",
    "    # Construct the prompt with a clear instruction to output only one answer\n",
    "    prompt = (\n",
    "        f\"{reasoning_prompt}\\n\\n\"\n",
    "        \"Context:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        f\"Question: {query_text}\\n\"\n",
    "        #\"Answer (only provide the answer, do not repeat the context):\"\n",
    "    )\n",
    "\n",
    "#     print(\"\\n\\nPrompt:\\n\",prompt)\n",
    "    # Use the pipeline to generate the answer\n",
    "    generated = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=100,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    # Remove the prompt from the generated text to isolate the answer\n",
    "    full_generated_text = generated[0]['generated_text']\n",
    "    final_answer = full_generated_text[len(prompt):].strip()\n",
    "    print(\"Generated Answer:\", final_answer)\n",
    "\n",
    "# Example query related to F-1 OPT and CPT\n",
    "# query_text = \"What is the eligibility criteria for F-1 OPT?\"\n",
    "# query_pinecone_and_generate_answer(query_text)\n",
    "\n",
    "\n",
    "# query_text = \"What is the eligibility criteria for H1B?\"\n",
    "# query_pinecone_and_generate_answer(query_text)\n",
    "\n",
    "\n",
    "# query_text = \"Can I work while I'm on F-1 status?\"\n",
    "# query_pinecone_and_generate_answer(query_text)\n",
    "query_text = \"How can I apply for H1B?\"\n",
    "query_pinecone_and_generate_answer(query_text)bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. query_text = \"What is the eligibility criteria for F-1 OPT?\"\n",
    "Generated Answer: Answer: An F-1 student who has attended an SEVP-certified college, university, conservatory, or seminary on a full-time basis for at least one academic year may be authorized for up to 12 months of OPT per education level. However, F-1 students who have one year or more of full-time curricular practical training are not eligible for OPT for that degree.\n",
    "\n",
    "#### 2. query_text = \"What is the eligibility criteria for H1B?\n",
    "Generated Answer: Answer: I am not authorized to provide advice on that topic. Please seek help from your DSO at Seattle University's International Student Center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.model.save_pretrained(\"/media/volume/gmhetre/saved_model_qwen\")\n",
    "pipe.tokenizer.save_pretrained(\"/media/volume/gmhetre/saved_model_qwen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Determine the best device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the saved model and tokenizer from the local directory\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"/media/volume/gmhetre/saved_model_qwen\",\n",
    "    tokenizer=\"/media/volume/gmhetre/saved_model_qwen\",\n",
    "    device=0 if device == \"cuda\" else -1\n",
    ")\n",
    "\n",
    "# Now you can use the pipeline to ask questions\n",
    "query_text = \"What is the eligibility criteria for F-1 OPT?\"\n",
    "generated = pipe(query_text, max_new_tokens=100, num_return_sequences=1, temperature=0.7, top_p=0.9)\n",
    "print(\"Generated Answer:\", generated[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can use the pipeline to ask questions\n",
    "query_text = \"What is the eligibility criteria for H1B?\"\n",
    "generated = pipe(query_text, max_new_tokens=100, num_return_sequences=1, temperature=0.7, top_p=0.9)\n",
    "print(\"Generated Answer:\", generated[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
