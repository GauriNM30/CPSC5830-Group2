{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! pip install -U \"huggingface_hub[cli]\" -q\n",
    "! pip install torch -q\n",
    "# ! pip install transformers_stream_generator einops tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!huggingface-cli login --token \"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\" #HUGGINGFACE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "\n",
    "# Access the API keys\n",
    "PINECONE_API_KEY = \"pcsk_5pi6rL_G3tGj8pTWTUTApkb8ujDfa9CszBJwTBwqsC97V8kMijqw9XE7Mhr5whJDRenCLv\"\n",
    "\n",
    "# Initialize Pinecone and embedding model\n",
    "api_key = PINECONE_API_KEY  # Replace with your API key\n",
    "pinecone = Pinecone(api_key=api_key)\n",
    "index_name = \"recursive-text-chunks\"\n",
    "\n",
    "index = pinecone.Index(index_name)\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Function to get the best available device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            torch.cuda.empty_cache()  # Clear unused memory\n",
    "            return torch.device(\"cuda\")\n",
    "        except RuntimeError:\n",
    "            print(\"GPU out of memory. Falling back to CPU.\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# Try initializing the model on GPU first, fallback to CPU if OOM error occurs\n",
    "try:\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"Qwen/Qwen2-7B-Instruct\",\n",
    "        device=0 if device.type == \"cuda\" else -1  # 0 for GPU, -1 for CPU\n",
    "    )\n",
    "    print(f\"Using device: {device}\")\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"GPU ran out of memory. Switching to CPU.\")\n",
    "        pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen2-7B-Instruct\", device=-1)\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Function to query Pinecone and generate an answer with the Llama model\n",
    "def query_pinecone_and_generate_answer(query_text, top_k=5):\n",
    "    global embedding_model, index\n",
    "    if 'embedding_model' not in globals() or 'index' not in globals():\n",
    "        raise ValueError(\"Embedding model and index must be defined before running the function.\")\n",
    "\n",
    "    # Query Pinecone to get relevant chunks\n",
    "    query_embedding = embedding_model.encode(query_text).tolist()\n",
    "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "\n",
    "    # Concatenate the top results into a context for the Llama model\n",
    "    context = \"\\n\".join([match['metadata']['text'] for match in results.get(\"matches\", [])])\n",
    "\n",
    "    # Read the reasoning prompt from file (make sure prompt.txt does not include extra Q&A instructions)\n",
    "    with open(\"prompt.txt\", \"r\") as f:\n",
    "        reasoning_prompt = f.read().strip()\n",
    "    \n",
    "    # Construct the prompt with a clear instruction to output only one answer\n",
    "    prompt = (\n",
    "        f\"{reasoning_prompt}\\n\\n\"\n",
    "        \"Context:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        f\"Question: {query_text}\\n\"\n",
    "        #\"Answer (only provide the answer, do not repeat the context):\"\n",
    "    )\n",
    "\n",
    "#     print(\"\\n\\nPrompt:\\n\",prompt)\n",
    "    # Use the pipeline to generate the answer\n",
    "    generated = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=100,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    # Remove the prompt from the generated text to isolate the answer\n",
    "    full_generated_text = generated[0]['generated_text']\n",
    "    final_answer = full_generated_text[len(prompt):].strip()\n",
    "    print(\"Generated Answer:\", final_answer)\n",
    "\n",
    "# Example query related to F-1 OPT and CPT\n",
    "# query_text = \"What is the eligibility criteria for F-1 OPT?\"\n",
    "# query_pinecone_and_generate_answer(query_text)\n",
    "\n",
    "\n",
    "# query_text = \"What is the eligibility criteria for H1B?\"\n",
    "# query_pinecone_and_generate_answer(query_text)\n",
    "\n",
    "\n",
    "# query_text = \"Can I work while I'm on F-1 status?\"\n",
    "# query_pinecone_and_generate_answer(query_text)\n",
    "query_text = \"How can I apply for H1B?\"\n",
    "query_pinecone_and_generate_answer(query_text)bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. query_text = \"What is the eligibility criteria for F-1 OPT?\"\n",
    "Generated Answer: Answer: An F-1 student who has attended an SEVP-certified college, university, conservatory, or seminary on a full-time basis for at least one academic year may be authorized for up to 12 months of OPT per education level. However, F-1 students who have one year or more of full-time curricular practical training are not eligible for OPT for that degree.\n",
    "\n",
    "#### 2. query_text = \"What is the eligibility criteria for H1B?\n",
    "Generated Answer: Answer: I am not authorized to provide advice on that topic. Please seek help from your DSO at Seattle University's International Student Center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pipe.model.save_pretrained(\"/media/volume/gmhetre/saved_model_qwen\")\n",
    "pipe.tokenizer.save_pretrained(\"/media/volume/gmhetre/saved_model_qwen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Determine the best device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the saved model and tokenizer from the local directory\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"/media/volume/gmhetre/saved_model_qwen\",\n",
    "    tokenizer=\"/media/volume/gmhetre/saved_model_qwen\",\n",
    "    device=0 if device == \"cuda\" else -1\n",
    ")\n",
    "\n",
    "# Now you can use the pipeline to ask questions\n",
    "query_text = \"What is the eligibility criteria for F-1 OPT?\"\n",
    "generated = pipe(query_text, max_new_tokens=100, num_return_sequences=1, temperature=0.7, top_p=0.9)\n",
    "print(\"Generated Answer:\", generated[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Now you can use the pipeline to ask questions\n",
    "query_text = \"What is the eligibility criteria for H1B?\"\n",
    "generated = pipe(query_text, max_new_tokens=100, num_return_sequences=1, temperature=0.7, top_p=0.9)\n",
    "print(\"Generated Answer:\", generated[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
