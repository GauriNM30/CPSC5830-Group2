{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas tqdm transformers numpy python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade jupyter ipywidgets -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import pipeline\n",
    "from typing import Dict\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "#!huggingface-cli login --token #HUGGINGFACE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is a high school diploma required for an F-1 v...</td>\n",
       "      <td>A high school diploma or its equivalent is gen...</td>\n",
       "      <td>Question Understanding\\nThe question asks whet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is it important to memorize my SEVIS ID?</td>\n",
       "      <td>It's crucial to know your SEVIS ID, as it's yo...</td>\n",
       "      <td>Question Understanding\\nThe question asks abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is proof of housing required at the port of en...</td>\n",
       "      <td>While proof of housing is not always required ...</td>\n",
       "      <td>Question Understanding\\nThe question asks whet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What document does a school provide for an F-1...</td>\n",
       "      <td>A school provides Form I-20, a Certificate of ...</td>\n",
       "      <td>Question Understanding\\nThe question asks abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What if I plan to do research collaboration wi...</td>\n",
       "      <td>If asked about potential research collaboratio...</td>\n",
       "      <td>Question Understanding\\nThe question concerns ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  Is a high school diploma required for an F-1 v...   \n",
       "1           Is it important to memorize my SEVIS ID?   \n",
       "2  Is proof of housing required at the port of en...   \n",
       "3  What document does a school provide for an F-1...   \n",
       "4  What if I plan to do research collaboration wi...   \n",
       "\n",
       "                                              Answer  \\\n",
       "0  A high school diploma or its equivalent is gen...   \n",
       "1  It's crucial to know your SEVIS ID, as it's yo...   \n",
       "2  While proof of housing is not always required ...   \n",
       "3  A school provides Form I-20, a Certificate of ...   \n",
       "4  If asked about potential research collaboratio...   \n",
       "\n",
       "                                           reasoning  \n",
       "0  Question Understanding\\nThe question asks whet...  \n",
       "1  Question Understanding\\nThe question asks abou...  \n",
       "2  Question Understanding\\nThe question asks whet...  \n",
       "3  Question Understanding\\nThe question asks abou...  \n",
       "4  Question Understanding\\nThe question concerns ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../dataset/cleaned_dataset_answer_improved_reasoned.csv\", encoding=\"utf-8\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTER_TO_INDEX = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "\n",
    "class Example:\n",
    "    def __init__(self, question: str, choice1: str, choice2: str, choice3: str, choice4: str):\n",
    "        self.question = question\n",
    "        self.choice1 = choice1\n",
    "        self.choice2 = choice2\n",
    "        self.choice3 = choice3\n",
    "        self.choice4 = choice4\n",
    "\n",
    "    @property\n",
    "    def choices(self):\n",
    "        return [self.choice1, self.choice2, self.choice3, self.choice4]\n",
    "\n",
    "def _base_prompt() -> str:\n",
    "    return \"\"\"Act as an expert legal assistant with comprehensive knowledge of statutory law and case precedent. Analyze the following legal question carefully, then select the correct answer from the given options through rigorous legal reasoning.\"\"\"\n",
    "\n",
    "def _format_choices(options: list[str]) -> str:\n",
    "    return \"\\n\".join(f\"({chr(65 + i)}) {choice}\" for i, choice in enumerate(options))\n",
    "\n",
    "def _build_question_section(example: Example) -> str:\n",
    "    return f\"\\n\\nQuestion: {example.question}\\nChoices:\\n{_format_choices(example.choices)}\"\n",
    "\n",
    "def _build_instructions() -> str:\n",
    "    return \"\"\"\\n\\n\n",
    "        Instructions:\n",
    "        1. Conduct thorough legal analysis of all options\n",
    "        2. Consider relevant statutes, regulations, and judicial interpretations\n",
    "        3. Identify potential ambiguities or counterarguments\n",
    "        4. Select only the BEST supported answer\n",
    "        5. Respond SOLELY with the correct letter (A-D)\n",
    "\n",
    "        Answer using this format:\n",
    "        [X]\"\"\"\n",
    "\n",
    "def _build_final_instruction() -> str:\n",
    "    return \"\\n\\nPlease reply only with the correct option, do not say anything else.\"\n",
    "\n",
    "def _prepare_examples(example: Example, no: int = 5) -> str:\n",
    "    filtered_df = df[df['Question'] != example.question].sample(frac=1)\n",
    "    examples = []\n",
    "    \n",
    "    for _, row in filtered_df.head(no).iterrows():\n",
    "        right_answer = str(row['Answer'])\n",
    "        option = [right_answer]\n",
    "        \n",
    "        distractors = df[df['Answer'] != right_answer]['Answer'].astype(str).unique()\n",
    "        if len(distractors) < 3:\n",
    "            raise ValueError(\"Not enough unique distractors in the DataFrame.\")\n",
    "        option += random.sample(list(distractors), 3)\n",
    "        \n",
    "        random.shuffle(option)\n",
    "        correct_letter = chr(option.index(right_answer) + 65)\n",
    "        \n",
    "        example_str = (\n",
    "            f\"\\n\\nQuestion: {row['Question']}\"\n",
    "            f\"\\nChoices:\\n{_format_choices(option)}\"\n",
    "            f\"\\nThe correct answer is ({correct_letter})\"\n",
    "        )\n",
    "        examples.append(example_str)\n",
    "    \n",
    "    return \"--- START OF EXAMPLES ---\\n\" + \"\".join(examples) + \"\\n\\n--- END OF EXAMPLES ---\\n\"\n",
    "\n",
    "def chain_of_thought_prompt(example: Example, max_new_tokens: int = 256) -> str:\n",
    "    prompt = _base_prompt() + _build_question_section(example)\n",
    "    prompt += f\"\\n\\nLet's analyze this step by step. First, understand the question. Next, evaluate each option in short (2-5) lines each. Also, you can generate up to {max_new_tokens} tokens to reason.\"\n",
    "\n",
    "    cot_output = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=pipe.tokenizer.eos_token_id,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        eos_token_id=pipe.tokenizer.eos_token_id,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    cot_reasoning = cot_output[0]['generated_text']\n",
    "    prompt = f\"{cot_reasoning}\\n\\nBased on the above, reasoning what is the single, most likely answer choice? \\nAnswer using this format:\\n[X]\"\n",
    "    prompt += _build_final_instruction()\n",
    "    return prompt\n",
    "\n",
    "def five_shot_prompt(example: Example, no: int = 5) -> str:\n",
    "    prompt = _base_prompt()\n",
    "    prompt += \"\\nHere are some example questions from experts. Answer the final question yourself, following the format of the previous questions exactly.\\n\"\n",
    "    prompt += _prepare_examples(example=example, no=no)\n",
    "    prompt += \"\\n\\nNow your turn. Choose the correct option that answers the below question.\\n\"\n",
    "    prompt += _build_question_section(example)\n",
    "    prompt += _build_instructions()\n",
    "    prompt += _build_final_instruction()\n",
    "    return prompt\n",
    "\n",
    "def zero_shot_prompt(example: Example) -> str:\n",
    "    prompt = _base_prompt() + _build_question_section(example)\n",
    "    prompt += _build_instructions()\n",
    "    prompt += _build_final_instruction()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete empty rows\n",
    "# df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove ',' from df[Answer]\n",
    "# df['Answer'] = df['Answer'].str.replace(',', '')\n",
    "# df.to_csv(\"../dataset/cleaned_dataset.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586f18f727394d67bc2c3c30b00ca5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "#model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "model_name = 'meta-llama/Llama-3.1-8B-Instruct' #\"meta-llama/Llama-3.2-3B-Instruct\"# \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "pipe = pipeline (\"text-generation\", model=model_name, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model pipeline\n",
    "#pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B-Instruct\", device='cuda')\n",
    "\n",
    "def runner(n=1, prompt_type='zero_shot_prompt'):\n",
    "   # model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    for i in tqdm(range(n), desc=\"Iterations\"):\n",
    "        results = []\n",
    "        \n",
    "        # Precompute paths and create directory once per iteration\n",
    "        model_folder = model_name.split('/')[1]\n",
    "        output_dir = f\"../results/base_model_eval/{prompt_type}/{model_folder}/\"\n",
    "        output_file = os.path.join(output_dir, f\"output_{i}.json\")\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Create directory if needed\n",
    "        \n",
    "        for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
    "            try:\n",
    "                question = row['Question']\n",
    "                right_answer = row['Answer']\n",
    "                # selecting distractions.\n",
    "                option = []\n",
    "                option.append(right_answer)\n",
    "                while len(option) < 4:\n",
    "                    distractor = df.sample(1)['Answer'].values[0]\n",
    "                    if distractor not in option and distractor != right_answer:\n",
    "                        option.append(distractor)\n",
    "\n",
    "                # print(f\"Question: {question}\")\n",
    "                # print(f\"Right Answer: {right_answer}\")\n",
    "                # print(f\"Options: {option}\")\n",
    "                # print(len(option)) \n",
    "                # break\n",
    "\n",
    "                # Create an example\n",
    "                random.shuffle(option)\n",
    "                # Right option is \n",
    "                right_option_index = option.index(right_answer)\n",
    "                right_option_letter = chr(ord('A') + right_option_index)\n",
    "                #print(f\"\\n\\nRight option index: {right_option_letter}\")\n",
    "                \n",
    "                example = Example(question, \n",
    "                                  option[0], \n",
    "                                  option[1], \n",
    "                                  option[2], \n",
    "                                  option[3]\n",
    "                                )\n",
    "                \n",
    "                # Depending on prompt_type, generate the prompt using the integrated functions\n",
    "                if prompt_type == 'zero_shot_prompt':\n",
    "                    prompt = zero_shot_prompt(example)\n",
    "                elif prompt_type == 'five_shot_prompt':\n",
    "                    n_examples = 5\n",
    "                    prompt =  five_shot_prompt(example, n_examples)\n",
    "                elif prompt_type == 'chain_of_thought_prompt':\n",
    "                    thinking_tokens = 512\n",
    "                    prompt = chain_of_thought_prompt(example, thinking_tokens)\n",
    "                else:\n",
    "                    # Default behavior (original prompt)\n",
    "                    print(\"SELECTING DEFAULT PROMPTING TECHNIQUE\")\n",
    "                    prompt = zero_shot_prompt(example)\n",
    "\n",
    "                #print(prompt)\n",
    "                #break\n",
    "                # Generate the model's response\n",
    "                response = pipe(\n",
    "                    prompt,\n",
    "                    max_new_tokens=15, # Preferably 15\n",
    "                    pad_token_id=pipe.tokenizer.eos_token_id,\n",
    "                    num_beams=5,\n",
    "                    early_stopping=True,\n",
    "                    eos_token_id=pipe.tokenizer.eos_token_id,\n",
    "                    temperature=0.1\n",
    "                )\n",
    "\n",
    "                # Extract generated text\n",
    "                generated_text = response[0][\"generated_text\"]\n",
    "                #print(generated_text)\n",
    "                answer_portion = generated_text.split(\"do not say anything else.\")[1] # do not say anything else. : is the ending of our prompt\n",
    "                match = re.search(r'[A-D]', answer_portion)\n",
    "                if match:\n",
    "                    model_answer_letter = match.group()\n",
    "                else:\n",
    "                    model_answer_letter = 'NA'\n",
    "                \n",
    "                                \n",
    "                if index == 0 and i==0:\n",
    "                    print(generated_text)\n",
    "                    print(\"Correct Ans: \", right_answer)\n",
    "                    print(\"Model Replied with: \", model_answer_letter)\n",
    "                    \n",
    "                is_correct = (right_option_letter and model_answer_letter) and right_option_letter.lower() == model_answer_letter.lower()\n",
    "\n",
    "                result = {\n",
    "                    \"iteration\": i,\n",
    "                    \"id\": index,\n",
    "                    \"model\": model_name,\n",
    "                    \"prompt_type\": prompt_type,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"question\": question,\n",
    "                    \"right_answer\": right_answer,\n",
    "                    \"right_answer_option\": right_option_letter,\n",
    "                    \"model_answer_letter\": model_answer_letter,\n",
    "                    \"generated_text\": generated_text,\n",
    "                    \"answer_portion\": answer_portion,\n",
    "                    \"is_correct\": str(is_correct)\n",
    "                }\n",
    "                results.append(result)\n",
    "\n",
    "                # Save progress every 50 successful completions\n",
    "                if len(results) % 50 == 0:\n",
    "                    print('saving')\n",
    "                    with open(output_file, 'w') as f:\n",
    "                        json.dump(results, f, indent=4)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {index}: {e}\")\n",
    "                print(generated_text)\n",
    "                continue \n",
    "\n",
    "        # Final save for remaining items after processing all rows\n",
    "        if len(results) > 0:\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "                \n",
    "        print(f\"Iteration {i} complete. Results saved to {output_file}\")\n",
    "        print(\"Evaluation complete. Results saved to output.json.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9454abfd29eb4f3db1e40285da733048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iterations:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd200a6f17745d3b6b3fa5d663a0d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing rows:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act as an expert legal assistant with comprehensive knowledge of statutory law and case precedent. Analyze the following legal question carefully, then select the correct answer from the given options through rigorous legal reasoning.\n",
      "Here are some example questions from experts. Answer the final question yourself, following the format of the previous questions exactly.\n",
      "--- START OF EXAMPLES ---\n",
      "\n",
      "\n",
      "Question: Can I do OPT after my masters if I already did it after my bachelors?\n",
      "Choices:\n",
      "(A) The designated school official (DSO) at the student's academic institution is authorized to approve an extension of their Form I-20, Certificate of Eligibility for Nonimmigrant Student Status, which is needed to maintain F-1 visa status. However, the F-1 visa itself, which allows entry into the U.S., cannot be extended from within the U.S.; students must apply for a new visa at a U.S. embassy or consulate abroad if their visa expires. The DSO's approval of the I-20 extension demonstrates that the student is maintaining status and is eligible to continue their studies.\n",
      "(B) Generally, you don't have to leave the U.S. to enroll in a new degree program while on OPT. You can transfer your SEVIS record to the new school by informing your Designated School Official (DSO) at your current school and following their instructions. Upon acceptance to the new program, the new school will issue you a new I-20, and your OPT employment must cease once the new program begins.\n",
      "(C) Yes, you are eligible for a new period of Optional Practical Training (OPT) upon completion of your master's degree, even if you previously utilized OPT after your bachelor's degree. Each higher degree level (Bachelor's, Master's, Doctorate) qualifies you for a separate and distinct OPT authorization. Therefore, graduating with a master's degree resets your OPT eligibility, allowing you to apply again.\n",
      "(D) Yes, you can work for a startup on OPT, but it must be a legitimate business operating legally and related to your field of study. For STEM OPT extensions, the startup must also be E-Verified. The employment should also offer training and learning objectives related to your degree.\n",
      "The correct answer is (C)\n",
      "\n",
      "Question: Can I travel during my semester on an F1 visa?\n",
      "Choices:\n",
      "(A) As you approach the unemployment limit on your OPT, reassess your job prospects and explore alternative options. If your chances of securing employment are slim, consider departing the U.S. before exceeding the limit to avoid violating your visa terms. Alternatively, explore options like changing to a different visa status or seeking advice from an immigration attorney to understand all possible paths forward.\n",
      "(B) While you can technically pay the SEVIS fee after scheduling your visa appointment, it's strongly recommended to pay it beforehand. The SEVIS fee payment confirmation is required during the visa interview, and paying in advance ensures you have it readily available. Additionally, allowing ample time for processing minimizes the risk of delays or complications with your visa application.\n",
      "(C) Yes, F1 students can typically travel outside the U.S. during semester breaks. However, it's crucial to have a valid passport, a valid F1 visa, and an I-20 form endorsed for travel by your Designated School Official (DSO) within the last year. If you plan to travel while classes are in session, you must obtain permission from your DSO and ensure your I-20 is properly endorsed.\n",
      "(D) To ensure you have the correct documentation before your interview, meticulously cross-reference your documents against the embassy's checklist. Double-check that you have your I-20, DS-160 confirmation, visa fee receipts, passport, and any required financial or academic records. Having all required documents readily available will help to enhance your visa application process and demonstrate preparedness.\n",
      "The correct answer is (C)\n",
      "\n",
      "Question: What is the filing fee for Form I-765?\n",
      "Choices:\n",
      "(A) Economic hardship employment is a work authorization granted by U.S. Citizenship and Immigration Services (USCIS) to F-1 students who can demonstrate unforeseen and severe financial difficulties that arose after they began their studies. These difficulties must make it impossible for the student to continue their education without working, and the employment authorization allows them to work off-campus to alleviate these financial burdens. To qualify, students must typically show they've made good-faith efforts to find on-campus employment and that accepting off-campus work will not interfere with their academic progress.\n",
      "(B) Demonstrate genuine interest by highlighting specific aspects of the university, such as unique programs, research opportunities, or faculty expertise, that resonate with your academic and career aspirations. Connect these features to your personal experiences and goals, illustrating how the university is the ideal place for you to thrive and contribute. Show that you've done your research and are making an informed choice beyond general rankings or reputation.\n",
      "(C) Eligibility for OPT is not automatic upon graduation for F-1 students. You must have maintained F-1 status for at least one full academic year and be pursuing a degree in a SEVP-certified school. Furthermore, the intended employment must be directly related to your field of study, and you must apply within the specified time frame.\n",
      "(D) The standard filing fee for Form I-765 is generally $520 for paper filings and $470 for online filings. However, there's a reduced fee of $260 available for adjustment of status applicants filing under category (c)(9) if Form I-485 was filed with a fee on or after April 1, 2024, and is still pending. It is crucial to verify the most current fee schedule on the USCIS website before submitting your application, as fees are subject to change.\n",
      "The correct answer is (D)\n",
      "\n",
      "Question: Can I use property valuation as proof of funds?\n",
      "Choices:\n",
      "(A) Traveling outside the U.S. while your OPT application is pending is risky because USCIS might consider it an abandonment of your application. Re-entry before OPT approval and receipt of your Employment Authorization Document (EAD) card could be problematic, potentially leading to denial of your OPT. It's generally advisable to remain in the U.S. until your OPT is approved and you have your EAD card to ensure smooth re-entry and maintain your application's validity.\n",
      "(B) Focus on demonstrating a stable and reliable income stream, providing bank statements and employment verification to showcase your ability to meet expenses. Supplement this with evidence of assets, such as property or investments, to further assure financial stability, acknowledging that while cash is preferred, a strong income record can be persuasive. Emphasize your history of responsible financial management and ability to cover costs without relying solely on a large lump sum.\n",
      "(C) While a property valuation demonstrates your asset worth, it's generally not accepted as proof of funds because it's not a liquid asset easily converted to cash. Financial institutions and other entities typically require bank statements, investment account statements, or similar documents showing readily accessible funds for verification. Consider exploring options like a home equity line of credit (HELOC) if you need to leverage your property's value for immediate funds.\n",
      "(D) No, receiving a Form 221(g) does not signify a permanent denial, but rather indicates that the consular officer requires further information or documentation to process your visa application. The form specifies the missing documents or necessary actions needed from your side. If you provide the required information and meet all eligibility criteria, your visa application can still be approved.\n",
      "The correct answer is (C)\n",
      "\n",
      "Question: What if the Officer asks me to speak about my academic or research interests?\n",
      "Choices:\n",
      "(A) Generally, USCIS does not issue a new EAD card for a Cap-Gap extension; instead, your expired OPT EAD card, along with your school-endorsed I-20 demonstrating the Cap-Gap extension, serves as proof of continued work authorization. Ensure that your DSO has properly endorsed your I-20 to reflect the Cap-Gap extension period. Employers can use these documents to verify your work authorization during the Cap-Gap period.\n",
      "(B) While on OPT, you can technically switch schools, but it will likely impact your OPT authorization. Your OPT is directly tied to your previously completed degree program, so enrolling in a new degree program generally terminates your current OPT authorization. To continue studying, you would need to pursue a new F-1 visa and potentially be eligible for CPT or OPT based on the new degree.\n",
      "(C) Emergency appointments are typically reserved for situations involving immediate threats to health, safety, or well-being. While personal reasons might sometimes necessitate urgent attention, appointment availability depends on the specific policies and resources of the service provider. It's best to contact the relevant authority directly to explain your situation and inquire about the possibility of an expedited appointment.\n",
      "(D) When discussing academic or research interests with an officer, provide a succinct and articulate summary that highlights your key achievements and future goals. Showcase your enthusiasm and demonstrate a clear understanding of your field, emphasizing the potential impact of your work. Be prepared to elaborate on specific projects or experiences that underscore your passion and expertise.\n",
      "The correct answer is (D)\n",
      "\n",
      "--- END OF EXAMPLES ---\n",
      "\n",
      "\n",
      "Now your turn. Choose the correct option that answers the below question.\n",
      "\n",
      "\n",
      "Question: Is a high school diploma required for an F-1 visa?\n",
      "Choices:\n",
      "(A) Yes, you can study in the U.S. even if your English is not proficient by enrolling in English as a Second Language (ESL) programs offered by many U.S. institutions. Upon successful completion of the ESL program and meeting the university's English proficiency requirements, you can then transition to a degree program after being accepted. You would typically need an F1 student visa for both ESL and degree programs.\n",
      "(B) To ensure a smooth transition, start your change of status process as soon as you are eligible. For H-1B visas, initiate the process well in advance of the April filing season for an October 1st start date. Green Card processes vary significantly, so early planning is crucial; consult an immigration attorney to determine the specific timelines and requirements for your situation and desired status.\n",
      "(C) F-1 visa holders can remain in the U.S. for the duration of their academic program, as indicated on their I-20 form, and while maintaining compliance with student status requirements. Additionally, upon completion of their program, they are typically granted a 60-day grace period to prepare for departure, transfer to another program, or change their visa status if eligible. It's crucial to note that failing to maintain student status can lead to visa termination and the loss of the authorized stay.\n",
      "(D) A high school diploma or its equivalent is generally required for an F-1 visa if you intend to pursue academic studies at a college, university, or other post-secondary institution. However, if you plan to enroll in a vocational or non-academic program, a high school diploma might not be mandatory, but meeting the educational requirements of that specific program is still essential. Always verify the specific admission requirements of the educational institution you plan to attend to ensure compliance with F-1 visa eligibility criteria.\n",
      "\n",
      "\n",
      "        Instructions:\n",
      "        1. Conduct thorough legal analysis of all options\n",
      "        2. Consider relevant statutes, regulations, and judicial interpretations\n",
      "        3. Identify potential ambiguities or counterarguments\n",
      "        4. Select only the BEST supported answer\n",
      "        5. Respond SOLELY with the correct letter (A-D)\n",
      "\n",
      "        Answer using this format:\n",
      "        [X]\n",
      "\n",
      "Please reply only with the correct option, do not say anything else. \n",
      "\n",
      "[X]  (D)  A high school diploma or its equivalent\n",
      "Correct Ans:  A high school diploma or its equivalent is generally required for an F-1 visa if you intend to pursue academic studies at a college, university, or other post-secondary institution. However, if you plan to enroll in a vocational or non-academic program, a high school diploma might not be mandatory, but meeting the educational requirements of that specific program is still essential. Always verify the specific admission requirements of the educational institution you plan to attend to ensure compliance with F-1 visa eligibility criteria.\n",
      "Model Replied with:  D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "Iteration 0 complete. Results saved to ../results/base_model_eval/five_shot_prompt/Llama-3.1-8B-Instruct/output_0.json\n",
      "Evaluation complete. Results saved to output.json.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ad626553b34a0cafbf4f57733f3e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing rows:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "Iteration 1 complete. Results saved to ../results/base_model_eval/five_shot_prompt/Llama-3.1-8B-Instruct/output_1.json\n",
      "Evaluation complete. Results saved to output.json.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e63af3d86674a789046f1da72a5d599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing rows:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "Iteration 2 complete. Results saved to ../results/base_model_eval/five_shot_prompt/Llama-3.1-8B-Instruct/output_2.json\n",
      "Evaluation complete. Results saved to output.json.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796c95c5859d45a5a602eaf6b0374037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing rows:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "Iteration 3 complete. Results saved to ../results/base_model_eval/five_shot_prompt/Llama-3.1-8B-Instruct/output_3.json\n",
      "Evaluation complete. Results saved to output.json.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3c305514ad4b7ca034b24ac8a8816b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing rows:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "saving\n",
      "Iteration 4 complete. Results saved to ../results/base_model_eval/five_shot_prompt/Llama-3.1-8B-Instruct/output_4.json\n",
      "Evaluation complete. Results saved to output.json.\n"
     ]
    }
   ],
   "source": [
    "runner(5, 'five_shot_prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../results/base_model_eval/zero_shot_prompt/Llama-3.1-8B-Instruct/output_1.json\n",
      "Total is_correct = True: 890\n",
      "Total is_correct = False: 110\n",
      "Total is_correct = NA: 102\n",
      "----------------------------------------\n",
      "File: ../results/base_model_eval/zero_shot_prompt/Llama-3.1-8B-Instruct/output_0.json\n",
      "Total is_correct = True: 874\n",
      "Total is_correct = False: 126\n",
      "Total is_correct = NA: 117\n",
      "----------------------------------------\n",
      "File: ../results/base_model_eval/zero_shot_prompt/Llama-3.1-8B-Instruct/output_4.json\n",
      "Total is_correct = True: 904\n",
      "Total is_correct = False: 93\n",
      "Total is_correct = NA: 87\n",
      "----------------------------------------\n",
      "File: ../results/base_model_eval/zero_shot_prompt/Llama-3.1-8B-Instruct/output_3.json\n",
      "Total is_correct = True: 885\n",
      "Total is_correct = False: 113\n",
      "Total is_correct = NA: 112\n",
      "----------------------------------------\n",
      "File: ../results/base_model_eval/zero_shot_prompt/Llama-3.1-8B-Instruct/output_2.json\n",
      "Total is_correct = True: 891\n",
      "Total is_correct = False: 107\n",
      "Total is_correct = NA: 102\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "prompt_type = 'zero_shot_prompt' #\"chain_of_thought_prompt\"  # five_shot_prompt # zero_shot_prompt\n",
    "#model_name = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "#model_name = \"deepseek-ai/deepseek-llm-7b-chat\" \n",
    "model_name =  \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Construct the directory path\n",
    "directory = f\"../results/base_model_eval/{prompt_type}/{model_name.split('/')[1]}/\" # When running on Jetstream\n",
    "#directory = f\"../results/base_model_eval/{prompt_type}/{model_name.split('/')[1]}/\" # When running in VS CODE\n",
    "\n",
    "# Find all JSON files matching output_*.json pattern\n",
    "json_files = glob.glob(os.path.join(directory, \"output_*.json\"))\n",
    "\n",
    "# Process each JSON file\n",
    "for file_path in json_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Count occurrences of is_correct = True and is_correct = False\n",
    "    true_count = sum(1 for item in data if item.get(\"is_correct\") == 'True') # model answer == actual answer\n",
    "    false_count = sum(1 for item in data if item.get(\"is_correct\") == 'False') # model answer != actual answer\n",
    "    na_count = sum(1 for item in data if item.get(\"model_answer_letter\") == 'NA') # Model was not able to answer/ Answer not found in response\n",
    "    \n",
    "    print(f\"File: {file_path}\")\n",
    "    print(f\"Total is_correct = True: {true_count}\")\n",
    "    print(f\"Total is_correct = False: {false_count}\")\n",
    "    print(f\"Total is_correct = NA: {na_count}\")\n",
    "    print(\"-\" * 40) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize the model pipeline\\n#pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B-Instruct\", device=\\'cuda\\')\\n\\ndef runner(n=1, prompt_type=\\'zero_shot_prompt\\'):\\n   # model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\\n    for i in tqdm(range(n), desc=\"Iterations\"):\\n        results = []\\n        for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\\n            try:\\n                question = row[\\'Question\\']\\n                right_answer = row[\\'Answer\\']\\n                # selecting distractions.\\n                option = []\\n                option.append(right_answer)\\n                while len(option) < 4:\\n                    distractor = df.sample(1)[\\'Answer\\'].values[0]\\n                    if distractor not in option and distractor != right_answer:\\n                        option.append(distractor)\\n\\n                # print(f\"Question: {question}\")\\n                # print(f\"Right Answer: {right_answer}\")\\n                # print(f\"Options: {option}\")\\n                # print(len(option)) \\n                # break\\n\\n                # Create an example\\n                random.shuffle(option)\\n                # Right option is \\n                right_option_index = option.index(right_answer)\\n                right_option_letter = chr(ord(\\'A\\') + right_option_index)\\n                #print(f\"\\n\\nRight option index: {right_option_letter}\")\\n                \\n                example = Example(question, \\n                                  option[0], \\n                                  option[1], \\n                                  option[2], \\n                                  option[3]\\n                                )\\n                \\n                # Depending on prompt_type, generate the prompt using the integrated functions\\n                if prompt_type == \\'zero_shot_prompt\\':\\n                    prompt = zero_shot_prompt(example)\\n                elif prompt_type == \\'five_shot_prompt\\':\\n                    n_examples = 5\\n                    prompt =  five_shot_prompt(example, n_examples)\\n                elif prompt_type == \\'chain_of_thought_prompt\\':\\n                    thinking_tokens = 512\\n                    prompt = chain_of_thought_prompt(example, thinking_tokens)\\n                else:\\n                    # Default behavior (original prompt)\\n                    print(\"SELECTING DEFAULT PROMPTING TECHNIQUE\")\\n                    prompt = zero_shot_prompt(example)\\n\\n                #print(prompt)\\n                #break\\n                # Generate the model\\'s response\\n                response = pipe(\\n                    prompt,\\n                    max_new_tokens=15, # Preferably 15\\n                    pad_token_id=pipe.tokenizer.eos_token_id,\\n                    num_beams=5,\\n                    early_stopping=True,\\n                    eos_token_id=pipe.tokenizer.eos_token_id,\\n                    temperature=0.1\\n                )\\n\\n                # Extract generated text\\n                generated_text = response[0][\"generated_text\"]\\n                #print(generated_text)\\n                answer_portion = generated_text.split(\"else.\")[1]\\n                match = re.search(r\\'[A-D]\\', answer_portion)\\n                if match:\\n                    model_answer_letter = match.group()\\n                else:\\n                    model_answer_letter = \\'NA\\'\\n                \\n                                \\n                if index == 0 and i==0:\\n                    print(generated_text)\\n                    print(\"Correct Ans: \", right_answer)\\n                    print(\"Model Replied with: \", model_answer_letter)\\n                    \\n                is_correct = (right_option_letter and model_answer_letter) and right_option_letter.lower() == model_answer_letter.lower()\\n\\n                result = {\\n                    \"iteration\": i,\\n                    \"id\": index,\\n                    \"model\": model_name,\\n                    \"prompt_type\": prompt_type,\\n                    \"prompt\": prompt,\\n                    \"question\": question,\\n                    \"right_answer\": right_answer,\\n                    \"right_answer_option\": right_option_letter,\\n                    \"model_answer_letter\": model_answer_letter,\\n                    \"generated_text\": generated_text,\\n                    \"answer_portion\": answer_portion,\\n                    \"is_correct\": str(is_correct)\\n                }\\n                results.append(result)\\n\\n            except Exception as e:\\n                print(f\"Error processing row {index}: {e}\")\\n                print(generated_text)\\n                continue \\n\\n        # Save the results\\n        if not os.path.exists(f\"results/base_model_eval/{prompt_type}/{model_name.split(\\'/\\')[1]}\"):\\n            os.makedirs(f\"results/base_model_eval/{prompt_type}/{model_name.split(\\'/\\')[1]}\")\\n\\n        with open(f\"results/base_model_eval/{prompt_type}/{model_name.split(\\'/\\')[1]}/output_{i}.json\", \"w\") as f:\\n                json.dump(results, f, indent=4)\\n    \\n        print(\"Evaluation complete. Results saved to output.json.\")    \\n               \\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Old code\n",
    "'''\n",
    "# Initialize the model pipeline\n",
    "#pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B-Instruct\", device='cuda')\n",
    "\n",
    "def runner(n=1, prompt_type='zero_shot_prompt'):\n",
    "   # model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    for i in tqdm(range(n), desc=\"Iterations\"):\n",
    "        results = []\n",
    "        for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
    "            try:\n",
    "                question = row['Question']\n",
    "                right_answer = row['Answer']\n",
    "                # selecting distractions.\n",
    "                option = []\n",
    "                option.append(right_answer)\n",
    "                while len(option) < 4:\n",
    "                    distractor = df.sample(1)['Answer'].values[0]\n",
    "                    if distractor not in option and distractor != right_answer:\n",
    "                        option.append(distractor)\n",
    "\n",
    "                # print(f\"Question: {question}\")\n",
    "                # print(f\"Right Answer: {right_answer}\")\n",
    "                # print(f\"Options: {option}\")\n",
    "                # print(len(option)) \n",
    "                # break\n",
    "\n",
    "                # Create an example\n",
    "                random.shuffle(option)\n",
    "                # Right option is \n",
    "                right_option_index = option.index(right_answer)\n",
    "                right_option_letter = chr(ord('A') + right_option_index)\n",
    "                #print(f\"\\n\\nRight option index: {right_option_letter}\")\n",
    "                \n",
    "                example = Example(question, \n",
    "                                  option[0], \n",
    "                                  option[1], \n",
    "                                  option[2], \n",
    "                                  option[3]\n",
    "                                )\n",
    "                \n",
    "                # Depending on prompt_type, generate the prompt using the integrated functions\n",
    "                if prompt_type == 'zero_shot_prompt':\n",
    "                    prompt = zero_shot_prompt(example)\n",
    "                elif prompt_type == 'five_shot_prompt':\n",
    "                    n_examples = 5\n",
    "                    prompt =  five_shot_prompt(example, n_examples)\n",
    "                elif prompt_type == 'chain_of_thought_prompt':\n",
    "                    thinking_tokens = 512\n",
    "                    prompt = chain_of_thought_prompt(example, thinking_tokens)\n",
    "                else:\n",
    "                    # Default behavior (original prompt)\n",
    "                    print(\"SELECTING DEFAULT PROMPTING TECHNIQUE\")\n",
    "                    prompt = zero_shot_prompt(example)\n",
    "\n",
    "                #print(prompt)\n",
    "                #break\n",
    "                # Generate the model's response\n",
    "                response = pipe(\n",
    "                    prompt,\n",
    "                    max_new_tokens=15, # Preferably 15\n",
    "                    pad_token_id=pipe.tokenizer.eos_token_id,\n",
    "                    num_beams=5,\n",
    "                    early_stopping=True,\n",
    "                    eos_token_id=pipe.tokenizer.eos_token_id,\n",
    "                    temperature=0.1\n",
    "                )\n",
    "\n",
    "                # Extract generated text\n",
    "                generated_text = response[0][\"generated_text\"]\n",
    "                #print(generated_text)\n",
    "                answer_portion = generated_text.split(\"else.\")[1]\n",
    "                match = re.search(r'[A-D]', answer_portion)\n",
    "                if match:\n",
    "                    model_answer_letter = match.group()\n",
    "                else:\n",
    "                    model_answer_letter = 'NA'\n",
    "                \n",
    "                                \n",
    "                if index == 0 and i==0:\n",
    "                    print(generated_text)\n",
    "                    print(\"Correct Ans: \", right_answer)\n",
    "                    print(\"Model Replied with: \", model_answer_letter)\n",
    "                    \n",
    "                is_correct = (right_option_letter and model_answer_letter) and right_option_letter.lower() == model_answer_letter.lower()\n",
    "\n",
    "                result = {\n",
    "                    \"iteration\": i,\n",
    "                    \"id\": index,\n",
    "                    \"model\": model_name,\n",
    "                    \"prompt_type\": prompt_type,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"question\": question,\n",
    "                    \"right_answer\": right_answer,\n",
    "                    \"right_answer_option\": right_option_letter,\n",
    "                    \"model_answer_letter\": model_answer_letter,\n",
    "                    \"generated_text\": generated_text,\n",
    "                    \"answer_portion\": answer_portion,\n",
    "                    \"is_correct\": str(is_correct)\n",
    "                }\n",
    "                results.append(result)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {index}: {e}\")\n",
    "                print(generated_text)\n",
    "                continue \n",
    "\n",
    "        # Save the results\n",
    "        if not os.path.exists(f\"results/base_model_eval/{prompt_type}/{model_name.split('/')[1]}\"):\n",
    "            os.makedirs(f\"results/base_model_eval/{prompt_type}/{model_name.split('/')[1]}\")\n",
    "\n",
    "        with open(f\"results/base_model_eval/{prompt_type}/{model_name.split('/')[1]}/output_{i}.json\", \"w\") as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "    \n",
    "        print(\"Evaluation complete. Results saved to output.json.\")    \n",
    "               \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
