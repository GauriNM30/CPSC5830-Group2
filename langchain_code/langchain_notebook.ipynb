{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple python code that uses Pinecone and also uses Lanhchain to develop and maintain chat history. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jenitza/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "from loguru import logger\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from railguard import RailGuard\n",
    "from pinecone_code import Pinecone_code\n",
    "\n",
    "# Langgraph and message classes\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "#https://python.langchain.com/docs/how_to/chatbots_memory/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanggraphAssistant:\n",
    "    def __init__(self):\n",
    "       \n",
    "        self.logger = logger\n",
    "        self.logger.add(\"logs/capstone.log\", format=\"{time} {level} {message}\", level=\"TRACE\")\n",
    "        load_dotenv()\n",
    "\n",
    "        # Initialize RailGuard for safety or evaluation\n",
    "        self.railguard = RailGuard()\n",
    "\n",
    "        # Initialize the Gemini model via ChatGoogleGenerativeAI\n",
    "        api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            temperature=0.3,\n",
    "            google_api_key=api_key\n",
    "        )\n",
    "\n",
    "        # Build the langgraph workflow\n",
    "        self.workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "        # Function that calls the model with current messages\n",
    "        def call_model(state: MessagesState):\n",
    "            response = self.llm.invoke(state[\"messages\"])\n",
    "            return {\"messages\": response}\n",
    "\n",
    "        # Define the graph edge and node\n",
    "        self.workflow.add_edge(START, \"model\")\n",
    "        self.workflow.add_node(\"model\", call_model)\n",
    "\n",
    "        # Initialize memory to store conversation history.\n",
    "        self.memory = MemorySaver()\n",
    "        self.app = self.workflow.compile(checkpointer=self.memory)\n",
    "\n",
    "        # Initialize your Pinecone interface\n",
    "        self.pinecone = Pinecone_code()\n",
    "\n",
    "        # Create a persistent thread id to preserve the conversation state\n",
    "        self.thread_id = uuid.uuid4()\n",
    "        self.config = {\"configurable\": {\"thread_id\": self.thread_id}}\n",
    "\n",
    "        # Track the number of questions to determine if context should be added.\n",
    "        self.question_count = 0\n",
    "\n",
    "    def clean_pinecone_response(self, pinecone_response):\n",
    "        return [match['metadata']['text'] for match in pinecone_response.get('matches', [])]\n",
    "\n",
    "    def ask_question(self, question: str) -> str:\n",
    "        # Evaluate the question with RailGuard first\n",
    "        railguard_response = self.railguard.railguard_eval(question)\n",
    "        \n",
    "        # Comment the not part if you want to test for follow up questions. Currently as the railguard prompt does not allow for follou up questions, this would not pass. Else modify rail guard prompt.\n",
    "        if not railguard_response:\n",
    "            return \"Please ask a valid question related to law.\"\n",
    "\n",
    "        self.question_count += 1\n",
    "        try:\n",
    "            # For the very first question, enrich with context from Pinecone\n",
    "            if self.question_count == 1:\n",
    "                pinecone_response = self.pinecone.query_pinecone(question, top_k=10) # Replace your RAG HERE\n",
    "                context = self.clean_pinecone_response(pinecone_response)\n",
    "                context_str = \"\\n\".join(context)\n",
    "                system_message = SystemMessage(\n",
    "                    content=f\"You are a legal assistant. Use this context to answer:\\n{context_str}\"\n",
    "                )\n",
    "                messages = [system_message, HumanMessage(content=question)]\n",
    "            else:\n",
    "                # For subsequent questions, just provide the human message.\n",
    "                messages = [HumanMessage(content=question)]\n",
    "            \n",
    "            result_messages = []\n",
    "            # Stream the conversation. The MemorySaver ensures that previous messages (and responses)\n",
    "            # are recalled using the same thread_id in the config.\n",
    "            for event in self.app.stream({\"messages\": messages}, self.config, stream_mode=\"values\"):\n",
    "                result_messages = event[\"messages\"]\n",
    "        \n",
    "           \n",
    "            return result_messages[-1].content\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Chat error: {e}\")\n",
    "            return \"Error processing your request\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer to first question: Optional Practical Training (OPT) is a type of work permission available for eligible F-1 students. It allows students to gain real-world work experience that is directly related to their field of study. It can be either pre-completion (before the program end date, part-time or full-time) or post-completion (after the program end date, at least 20 hours per week or full-time). A student is typically eligible for 12 months of OPT per higher level of study.\n",
      "Answer to second question: Please ask a valid question related to law.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    assistant = LanggraphAssistant()\n",
    "\n",
    "    # First question \n",
    "    answer1 = assistant.ask_question(\"What is Optional Practical Training (OPT)?\")\n",
    "    print(\"Answer to first question:\", answer1)\n",
    "\n",
    "    # Second question asking about the first question to see history preserved.\n",
    "    answer2 = assistant.ask_question(\"What was my first question?\")\n",
    "    print(\"Answer to second question:\", answer2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
